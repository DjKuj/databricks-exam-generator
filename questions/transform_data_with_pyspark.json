{
    "0":{
       "questions":"A data engineer has a DataFrame events_df that has been registered against an external JSON file. They need to access the field date within events_df. The events_df DataFrame has the following schema:\n\ndate string\nmonth string\nevent_type string\n\nWhich of the following approaches can the data engineer use to accomplish this? Select one response.\n",
       "choices":[
          "They can index the query by subfield using events[date] syntax.",
          "They can use from_json() to parse the column for date.",
          "They can use . syntax to access date in events_df.",
          "They can use date.* to pull out the subfields of events_df into their own columns.",
          "They can use : syntax to access date in events_df."
       ],
       "expected_answers":1,
       "answers":5
    },
    "1":{
       "questions":"A data engineer wants to extract lines as raw strings from a text file.\n\nWhich of the following SQL commands accomplishes this task? Select one response.\n",
       "choices":[
          "SELECT * FROM `${dbfs:/mnt/datasets}/001.txt` as TEXT;",
          "SELECT text(*) FROM ${dbfs:/mnt/datasets}/001.txt`;",
          "SELECT * FROM text.`${dbfs:/mnt/datasets}/001.txt`;",
          "SELECT (*) FROM ${dbfs:/mnt/datasets}/001.txt`;",
          "SELECT * FROM `${dbfs:/mnt/datasets}/001.txt`;"
       ],
       "expected_answers":1,
       "answers":3
    },
    "2":{
       "questions":"A data engineer needs to create and register a user-defined function (UDF) CREATE_USER using the Python function createUser and apply it to array column username in the table users.They have the following lines of code.\n\nLines of code:\n\n1. spark.sql(\"SELECT createUser(username) AS user FROM users\") \n2. spark.sql(\"SELECT CREATE_USER(username) AS user FROM users\") \n 3. spark.udf.register(\"CREATE_USER\", createUser) \n4. spark.udf.register(createUser(username)) \n\n.In what order do the lines of code above need to be run in a Python session in order to accomplish this? Select one response.\n",
       "choices":[
          "2",
          "3,1",
          "4,1",
          "3,2",
          "4,2"
       ],
       "expected_answers":1,
       "answers":4
    },
    "3":{
       "questions":"A data engineer has a DataFrame events_df that has been registered against an external JSON file. The nested JSON fields have already been converted into struct types. The data engineer now needs to flatten the struct fields back into individual columns for the field event_type. The events_df DataFrame has the following schema:\n\ndate string\nmonth string\nevent_type StructType&lt;id string, size int&gt;\n\nWhich of the following approaches allows the data engineer to retrieve id within event_type? Select one response.\n",
       "choices":[
          "They can use event_type.* to pull out id into its own column.",
          "They can index the DataFrame by id.",
          "They can use : syntax to access id in event_type.",
          "They can use from_json() to parse the columns for id.",
          "They can use . syntax to access id in event_type."
       ],
       "expected_answers":1,
       "answers":5
    },
    "4":{
       "questions":"A data engineer needs to query a JSON file whose location is represented by the variable path.\n\nWhich of the following commands do they need to use? Select one response.\n",
       "choices":[
          "RETURN json.`${path}`;",
          "SHOW TABLE json.`${path}`;",
          "SELECT * FROM json.`${path}`;",
          "SELECT * FROM path LOCATION `${path}`;",
          "DISPLAY TABLE json.`${path}`;"
       ],
       "expected_answers":1,
       "answers":3
    },
    "5":{
       "questions":"A data engineer is using the following code block to create a table using an external CSV file as its source. They need to specify that the fields are separated by | and that there is a header.\n\n\nCREATE TABLE IF NOT EXISTS sales_csv\n(order_id LONG, email STRING, transactions_timestamp LONG, total_item_quantity INTEGER, purchase_revenue_in_usd DOUBLE, unique_items INTEGER, items STRING)\nUSING CSV\n_____\nLOCATION \"${dbfs:/mnt/datasets}\"\n\nWhich of the following correctly fills in the blank for the table options? Select one response.\n",
       "choices":[
          "KEYS (\n   header = \"true\",\n   delimiter = \"|\"\n)",
          "OPTIONS (\n   header = \"true\",\n   delimiter = \"|\"\n)",
          "COMMENTS (\n   header = \"true\",\n   delimiter = \"|\"\n)",
          "VALUES (\n   header = \"true\",\n   delimiter = \"|\"\n)",
          "TBLPROPERTIES (\n   header = \"true\",\n   delimiter = \"|\"\n)"
       ],
       "expected_answers":1,
       "answers":2
    },
    "6":{
       "questions":"A data engineer is using the following query to confirm that each unique string value in the phone_number column in the usersDF DataFrame is associated with at most one user_id. They want the query to return true if each phone_number is associated with at most 1 user_id. When they run the query, they notice that the query is not returning the expected result.\n\n\nfrom pyspark.sql.functions import countDistinct\n\nusersDF\n.groupBy(\"phone_number\")\n.agg(countDistinct(\"user_id\").alias(\"unique_user_ids\"))\n\nWhich of the following explains why the query is not returning the expected result? Select one response.\n",
       "choices":[
          "A .select(max(\"unique_user_ids\") &lt;= 1)function needs to be added after the .agg() function.",
          ".groupBy(\"phone_number\") needs to be changed to .countDistinct(phone_number).",
          "A .dropDuplicates() statement needs to be added after the .agg() function.",
          "A .merge statement on row_count == count(phone_number) needs to be added after the groupBy() function.",
          ".groupBy(\"phone_number\") needs to be changed to count(*).when(user_id != null)."
       ],
       "expected_answers":1,
       "answers":1
    },
    "7":{
       "questions":"Which of the following identifies a performance challenge associated with Python UDFs? Select three responses",
       "choices":[
          "Python UDFs cannot contain column aggregations or data optimizations.",
          "Python UDFs cannot be optimized by the Catalyst Optimizer.",
          "Python UDFs have interprocess communication overhead between the executor and a Python interpreter running on each worker node.",
          "Python UDFs cannot use vector or scalar transformations.",
          "Python UDFs have to deserialize row data from Spark's native binary format to pass to the UDF, and the results are serialized back into Spark's native format."
       ],
       "expected_answers":3,
       "answers":[
          2,
          3,
          5
       ]
    },
    "8":{
       "questions":"Which of the following statements about querying tables defined against external sources is true? Select one response.\n",
       "choices":[
          "When defining tables or queries against external data sources, the performance guarantees associated with Delta Lake and Lakehouse cannot be guaranteed.",
          "When defining tables or queries against external data sources, older cached versions of the table are automatically added to the event log.",
          "None of these statements about external table behavior are true.",
          "When defining tables or queries against external data sources, the storage path, external location, and storage credential are displayed for users who have been granted USAGE access to the table.",
          "When defining tables or queries against external data sources, older cached versions of the table are automatically deleted."
       ],
       "expected_answers":1,
       "answers":1
    },
    "9":{
       "questions":"A data engineer has a DataFrame with string column email_address. They are using a regular expression that returns a string with a matching pattern when it is in the following format:\nuser.address@domain.com\nWhich of the following lines of code creates a new column domain that contains the domain from the email_address column? Select one response.\n",
       "choices":[
          ".withColumn(\"domain\", regexp_extract(\"email_address\", \"(?&lt;=@).+\", 0))",
          ".withColumn(\"domain\", collect_set(\"email_address\", \"(?&lt;=@).+\", 0))",
          ".withColumn(\"domain\", flatten(\"email_address\", \"(?&lt;=@).+\", 0))",
          ".withColumn(\"domain\", array_distinct(\"email_address\", \"(?&lt;=@).+\", 0))",
          "None of these responses correctly extract the domain."
       ],
       "expected_answers":1,
       "answers":1
    },
    "10":{
       "questions":"A data engineer has created the following Spark DataFrame sales_df that joins the previously created table sales with the Spark DataFrame items_df when sales and items_df have matching values in the sales_id column in both data objects.\n\n\n\nsales_df = (spark\n.table(\"sales\")\n.withColumn(\"item\", explode(\"items\"))\n)\n\nitems_df = spark.table(\"item_lookup\")\n\nitem_purchasesDF = (sales_df\n______________________)\n\nWhich of the following lines of code correctly fills in the blank? Select one response.\n",
       "choices":[
          ".outerJoin(items_df, sales.sales_id == items_df.sales_id)",
          ".merge(items_df, sales_df, on = \"item_id\")",
          ".join(items_df, sales.sales_id == items_df.sales_id, how = \"cross\")",
          ".join(items_df, sales.sales_id == items_df.sales_id)",
          ".innerJoin(items_df, sales_df.sales_id == items_df.sales_id)"
       ],
       "expected_answers":1,
       "answers":4
    },
    "11":{
       "questions":"A data engineer has created a DataFrame exploded_eventsDF created from the table exploded_events defined here:\n\nCREATE TABLE events (user_id string, event_name string, item_id string, events struct&lt;coupon:string, event_id:string, event_revenue:double&gt;);\n\nThey are using the following code with multiple array transformations to return a new DataFrame that shows the unique collection of the columns event_name and items.\n\n\n\nfrom pyspark.sql.functions import array_distinct, collect_set, flatten\n\nexploded_eventsDF\n.groupby(\"user_id\")\n.agg(collect_set(\"event_name\"),\n_____\n\nWhich of the following lines of code fills in the blank to create the column event_history as a unique collection of events? Select one response.\n",
       "choices":[
          "array_distinct(extract(collect_set(events.event_id))).alias(\"event_history\")",
          "flatten(extract(events.event_id)).alias(\"event_history\")",
          "flatten(collect_set(explode(events:event_id))).alias(\"event_history\")",
          "array_distinct(flatten(collect_set(\"events.event_id\"))).alias(\"event_history\")",
          "flatten(array_distinct(events[event_id])).alias(\"event_history\")"
       ],
       "expected_answers":1,
       "answers":4
    },
    "12":{
       "questions":"A data engineer has a table records with a column email. They want to check if there are null values in the email column.\n\nWhich of the following approaches accomplishes this? Select one response.\n",
       "choices":[
          "They can check if there is at least one record where email is null by creating a data expectation to drop null values.",
          "They can check if there is at least one record where email is null using SELECT DISTINCT records.",
          "They can check if there is at least one record where email is null by adding a filter for when email IS NULL to a SELECT statement.",
          "They can check if there is at least one record where email is null by running a regular expression function on email to filter out null values.",
          "They can check if there is at least one record where email is null by pivoting the table on null values."
       ],
       "expected_answers":1,
       "answers":3
    },
    "13":{
       "questions":"A data engineer has created a custom Python function that returns an input variable of type double after it has been incremented by 1.\n\nWhich of the following identifies the most efficient type of UDF the engineer can use in terms of complexity, as well as the correct method to specify the type of UDF? Select one response.",
       "choices":[
          "The data engineer can specify that the UDF is vectorized using @pandas_udf(\"double\", PandasUDFType.SCALAR) syntax.",
          "The data engineer can specify that the UDF has row-at-a-time execution using @pandas_udf(\"double\", PandasUDFType.SCALAR) syntax.",
          "The data engineer can specify that the UDF has row-at-a-time execution using @pandas_udf(\"double\", PandasUDFType.VECTORIZED) syntax.",
          "The data engineer can specify that the UDF is vectorized using type hints.",
          "The data engineer can specify that the UDF is vectorized using @pandas_udf(\"double\", PandasUDFType.VECTORIZED) syntax."
       ],
       "expected_answers":1,
       "answers":1
    },
    "14":{
       "questions":"A data engineer is registering a table in Databricks using the table users from an external SQL database. One of their colleagues gives them the following code to register the table. However, when the data engineer runs the code, they notice an error.\n\n\nCREATE TABLE users_jdbc\nUSING JDBC\nOPTIONS ( url = \"jdbc:sqlite:${DA.paths.ecommerce_db}\")\n\nWhich of the following correctly identifies why running the code is resulting in an error? Select one response.",
       "choices":[
          "None of these responses correctly identify the cause of the error.",
          "USING JDBC needs to be changed to USING SQL.",
          "CREATE TABLE needs to be changed to CREATE JDBC TABLE.",
          "A username and password need to be added to OPTIONS.",
          "The line dbtable = \"users\" needs to be added to OPTIONS."
       ],
       "expected_answers":1,
       "answers":5
    },
    "15":{
       "questions":"Which of the following lines of code counts null values in the column email from the DataFrame usersDF? Select two responses.",
       "choices":[
          "usersDF.drop()",
          "usersDF.distinct()",
          "usersDF.selectExpr(\"count_if(email IS NULL)\")",
          "usersDF.count().dropna()",
          "usersDF.where(col(\"email\").isNull()).count()"
       ],
       "expected_answers":2,
       "answers":[
          3,
          5
       ]
    },
    "16":{
       "questions":"A data engineer needs a reference to the results of a query that can be referenced across multiple queries within the scope of the environment session. The data engineer does not want the reference to exist outside of the scope of the environment session.\n\nWhich of the following approaches accomplishes this without explicitly dropping the data object? Select one response.\n",
       "choices":[
          "They can store the results of their query within a reusable user-defined function (UDF).",
          "They can store the results of their query within a common table expression (CTE).",
          "They can store the results of their query within a view.",
          "They can store the results of their query within a table.",
          "They can store the results of their query within a temporary view."
       ],
       "expected_answers":1,
       "answers":5
    },
    "17":{
       "questions":"A data engineer has the following query, where path is a variable that represents the location of a directory.\n\nQuery:\nSELECT * FROM csv.`${path}`;\n",
       "choices":[
          "The query streams data from a directory of CSV files into a table.",
          "The query displays the underlying file contents of a directory of CSV files.",
          "The query loads the contents of a directory of CSV files from a source table to a target table.",
          "The query displays the metadata of a directory of CSV files.",
          "The query converts a directory of files into CSV format."
       ],
       "expected_answers":1,
       "answers":2
    },
    "18":{
       "questions":"A data engineer is using the following code block to create and register a function that returns the first letter of the string email. Another data engineer points out that there is a more efficient way to do this.\n\nfrom pyspark.sql.functions import udf\n\n@udf(\"string\")\ndef first_letter_function(email: str) -&gt; str:\nreturn email[0]\n\nfirst_letter_udf = spark.udf.register(\"sql_udf\", first_letter_function)\n\nWhich of the following identifies how the data engineer can eliminate redundancies in the code? Select one response.\n",
       "choices":[
          "They can eliminate the parameters in the function declaration.",
          "They can eliminate the statement that registers the function.",
          "They can eliminate \"sql_udf\" from the statement that registers the function.",
          "They can eliminate the return statement at the end of the function.",
          "They can eliminate the import statement in the beginning of the code block."
       ],
       "expected_answers":1,
       "answers":2
    },
    "19":{
       "questions":"Which of the following statements about the difference between views and temporary views are correct? Select two responses.",
       "choices":[
          "Temporary views skip persisting the definition in the underlying metastore. Views have metadata that can be accessed in the view\u2019s directory.",
          "Temporary views have names that must be qualified. Views have names that must be unique.",
          "Temporary views are session-scoped and dropped when the Spark session ends. Views can be accessed after the session ends.",
          "Temporary views do not contain a preserved schema. Views are tied to a system preserved temporary schema global_temp.",
          "Temporary views reside in the third layer of Unity Catalog\u2019s three-level namespace\u00a0Views lie in the metastore."
       ],
       "expected_answers":2,
       "answers":[
          1,
          3
       ]
    },
    "20":{
       "questions":"A data engineer has a table high_temps with the following schema, where avg_high_temp represents the monthly average high temperatures for each unique year-month combination.\n\nyear string\nmonth string\navg_high_temp string\n\nThey need to reformat the data with years as the primary record key and months as the columns. The existing average high temperature value for each year-month combination needs to be in the month columns.\n\nHow can the data engineer accomplish this? Select one response.\n",
       "choices":[
          "The data engineer can rotate the data from long to wide format using the .groupBy()clause.",
          "The data engineer can rotate the data from wide to long format using the .transform() clause.",
          "The data engineer can rotate the data from long to wide format using the .pivot() function.",
          "The data engineer can rotate the data from wide to long format using the .pivot() function.",
          "The data engineer can rotate the data from long to wide format using the .transform()clause."
       ],
       "expected_answers":1,
       "answers":3
    },
    "21":{
       "questions":"Which of the following commands returns a new DataFrame from the DataFrame usersDF without duplicates? Select one response.",
       "choices":[
          "usersDF.drop()",
          "usersDF.distinct()",
          "usersDF.select(*)",
          "usersDF.count().dropna()",
          "usersDF.groupBy(nulls)"
       ],
       "expected_answers":1,
       "answers":2
    },
    "22":{
       "questions":"A data engineer has a query that directly updates the files underlying the external table emails.\n\nWhich of the following correctly describes how to retrieve the number of rows in the updated table? Select one response.\n",
       "choices":[
          "REFRESH TABLE emails;\nSELECT DISTINCT_COUNT(*) FROM emails;",
          "REFRESH TABLE emails;\nSELECT COUNT(*) FROM emails WHEN UPDATED = TRUE;",
          "REFRESH TABLE emails;\nSELECT COUNT(*) FROM emails AS OF VERSION 1;",
          "REFRESH TABLE emails;\nSELECT COUNT(*) FROM emails;",
          "REFRESH TABLE emails;\nSELECT DISTINCT_COUNT(*) FROM emails AS OF VERSION 1;"
       ],
       "expected_answers":1,
       "answers":4
    },
    "23":{
       "questions":"A senior data engineer has registered the Python function create_users to be used by the rest of their team. They have written the function in Python 3.6. Another data engineer wants to know what the expected inputs are.\nWhich of the following is considered a best practice to do this? Select one response.\n",
       "choices":[
          "The data engineer can add a comment to the table properties to clarify the input and return types of the function.",
          "The data engineer can declare the function with @udf(\"string\") syntax to specify the input and return types of the function.",
          "The data engineer can add the input and output types to the table using @options()",
          "The data engineer can use type hints to clarify the input and return types of the function.",
          "The data engineer can add a return string line to the end of their UDF to specify the input and return types of the function."
       ],
       "expected_answers":1,
       "answers":4
    },
    "24":{
       "questions":"A data engineer needs to extract the calendar date and time in human readable format from a DataFrame containing the timestamp column user_last_touch_timestamp.\nWhich of the following lines of code correctly fills in the blank by adding the column end_date of type date in human readable format? Select one response.",
       "choices":[
          ".withColumn(\"end_date\", CAST(user_last_touch_timestamp) as date_format)",
          ".withColumn(\"end_date\", date_format(\"user_last_touch_timestamp\", \"MMM d, yyyy\"))",
          ".withColumn(date_format(\"end_date\"), user_last_touch_timestamp, \"HH:mm:ss\")",
          ".withColumn(date_time(\"end_date\"),user_last_touch_timestamp, \"HH:mm:ss\")",
          ".withColumn(date_time(\"end_date\"), user_last_touch_timestamp, \"MMM d, yyyy\")"
       ],
       "expected_answers":1,
       "answers":2
    }
 }